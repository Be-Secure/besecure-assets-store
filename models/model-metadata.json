[
  {
    "id": 1,
    "bes_tracking_id": 1,
    "issue_url": "",
    "type": "Classic",
    "name": "bes-image-classification",
    "organization": "Be-Secure",
    "description": "Reference implementations of an MNIST model for image classification, specifically focusing on handwritten digit data. The implementation utilizes the MNIST dataset, which is part of TensorFlow Datasets",
    "created_date": "2023-11-30",
    "model_url": "https://github.com/asa1997/ml-evasion/raw/main/model.zip",
    "data_url": "https://github.com/asa1997/ml-evasion/raw/main/data.zip",
    "label_url": "https://github.com/asa1997/ml-evasion/raw/main/label.zip",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST", "Fuzz Test"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 2,
    "bes_tracking_id": 2,
    "issue_url": "",
    "type": "Classic",
    "name": "structured-data-classification-grn-vsn",
    "organization": "keras-io",
    "description": "This model is built using two important architectural components proposed by Bryan Lim et al. in Temporal Fusion Transformers (TFT) for Interpretable Multi-horizon Time Series Forecasting called GRN and VSN which are very useful for structured data learning tasks.",
    "created_date": "2022-07-04",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for binary classification task to determine whether a person makes over $500K a year or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn/discussions"
  },
  {
    "id": 3,
    "bes_tracking_id": 3,
    "issue_url": "",
    "type": "Classic",
    "name": "xgboost-example",
    "organization": "scikit-learn",
    "description": "This is an XGBoost model trained to predict daily alcohol consumption of students.",
    "created_date": "2023-01-20",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/scikit-learn/xgboost-example",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 4,
    "bes_tracking_id": 4,
    "issue_url": "",
    "type": "Classic",
    "name": "TF_Decision_Trees",
    "organization": "keras-io",
    "description": "Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model.",
    "created_date": "2022-02-15",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/TF_Decision_Trees",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/TF_Decision_Trees/discussions"
  },
  {
    "id": 5,
    "bes_tracking_id": 5,
    "issue_url": "",
    "type": "Classic",
    "name": "tab_transformer",
    "organization": "keras-io",
    "description": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "created_date": "2022-06-11",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/tab_transformer",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/tab_transformer/discussions"
  },
  {
    "id": 6,
    "bes_tracking_id": 6,
    "issue_url": "",
    "type": "Classic",
    "name": "imbalanced_classification",
    "organization": "keras-io",
    "description": "credit card fraud detection",
    "created_date": "2022-06-13",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/imbalanced_classification",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "The trained model is used to detect of a specific transaction is fraudulent or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/imbalanced_classification/discussions"
  },
  {
    "id": 7,
    "bes_tracking_id": 7,
    "issue_url": "",
    "type": "Classic",
    "name": "breast_cancernb8gjv4n-diagnosis-classification",
    "organization": "merve",
    "description": "Baseline Model trained on breast_cancernb8gjv4n to apply classification on diagnosis",
    "created_date": "2022-07-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification/discussions"
  },
  {
    "id": 8,
    "bes_tracking_id": 8,
    "issue_url": "",
    "type": "Classic",
    "name": "tabular-playground",
    "organization": "demo-org",
    "description": "This is a DecisionTreeClassifier model built for Kaggle Tabular Playground Series August 2022, trained on supersoaker production failures dataset.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/demo-org/tabular-playground",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/demo-org/tabular-playground/discussions"
  },
  {
    "id": 9,
    "bes_tracking_id": 9,
    "issue_url": "",
    "type": "Classic",
    "name": "modeltest",
    "organization": "vendorabc",
    "description": "This is a HistGradientBoostingClassifier model trained on breast cancer dataset. It's trained with Halving Grid Search Cross Validation, with parameter grids on max_leaf_nodes and max_depth.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/vendorabc/modeltest",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/vendorabc/modeltest/discussions"
  },
  {
    "id": 10,
    "bes_tracking_id": 10,
    "issue_url": "",
    "type": "Classic",
    "name": "emb-gam-dino",
    "organization": "Ramos-Ramos",
    "description": "This is a LogisticRegressionCV model trained on averages of patch embeddings from the Imagenette dataset. This forms the GAM of an Emb-GAM extended to images.",
    "created_date": "2022-10-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/Ramos-Ramos/emb-gam-dino",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/Ramos-Ramos/emb-gam-dino/discussions"
  },
  {
    "id": 11,
    "bes_tracking_id": 11,
    "issue_url": "",
    "type": "Classic",
    "name": "ml-generation-failure-prediction",
    "organization": "moro23",
    "description": "ML classification model to predict or identify failures in a generato",
    "created_date": "2022-12-17",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/moro23/ml-generation-failure-prediction",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/moro23/ml-generation-failure-prediction/discussions"
  },
  {
    "id": 12,
    "bes_tracking_id": 12,
    "issue_url": "",
    "type": "LLM",
    "name": "MathCoder",
    "organization": "Shanghai AI Laboratory",
    "description": "MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.",
    "created_date": "2023-10-05",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": ["GPT-4", "LLaMA 2"],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 13,
    "bes_tracking_id": 13,
    "issue_url": "",
    "type": "LLM",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.",
    "created_date": "2023-09-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://falconllm.tii.ae/falcon-models.html",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": ["RefinedWeb"],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "id": 14,
    "bes_tracking_id": 14,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-18",
    "organization": "microsoft",
    "description": "ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-18",
    "modality": "",
    "analysis": "",
    "size": "11.7M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 15,
    "bes_tracking_id": 15,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-arabic-camelbert-ca-pos-glf",
    "organization": "CAMeL-Lab",
    "description": "CAMeLBERT-CA POS-GLF Model is a Gulf Arabic POS tagging model that was built by fine-tuning the CAMeLBERT-CA model. For the fine-tuning, we used the Gumar dataset.",
    "created_date": "2021-10-18",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 16,
    "bes_tracking_id": 16,
    "issue_url": "",
    "type": "LLM",
    "name": "all-MiniLM-L6-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-08-31",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "1B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "as a sentence and short paragraph encoder",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 17,
    "bes_tracking_id": 17,
    "issue_url": "",
    "type": "LLM",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-06-09",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 18,
    "bes_tracking_id": 18,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-NER",
    "organization": "dslim",
    "description": "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
    "created_date": "2020-12-12",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/dslim/bert-base-NER",
    "modality": "",
    "analysis": "",
    "size": "108M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 19,
    "bes_tracking_id": 19,
    "issue_url": "",
    "type": "LLM",
    "name": "vit-base-patch16-224",
    "organization": "google",
    "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch.",
    "created_date": "2021-03-24",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/google/vit-base-patch16-224",
    "modality": "image; image",
    "analysis": "",
    "size": "86.6M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 20,
    "bes_tracking_id": 20,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-50",
    "organization": "microsoft",
    "description": "VResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-50",
    "modality": "image; image",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 21,
    "bes_tracking_id": 21,
    "issue_url": "",
    "type": "LLM",
    "name": "collaborative-filtering-movielens",
    "organization": "keras-io",
    "description": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "created_date": "2022-06-02",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/collaborative-filtering-movielens",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "CC0 1.0",
    "intended_uses": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/collaborative-filtering-movielens/discussions"
  }
]
