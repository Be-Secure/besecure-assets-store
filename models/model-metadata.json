[
  {
    "id": 1,
    "bes_tracking_id": 1,
    "issue_url": "",
    "type": "LLM",
    "name": "DeepSeek-R1:7b",
    "organization": "deepseek-ai",
    "description": "DeepSeek-R1:7b, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.",
    "created_date": "2025-02-18",
    "model_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/deepseek-ai/DeepSeek-R1",
    "model_card": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "modality": "text; text",
    "analysis": "",
    "size": "7B params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Security Benchmark"],
    "access": "open",
    "license": "mit",
    "intended_uses": "DeepSeek-R1 is designed for various NLP tasks, including text generation, reasoning, code assistance, translation, summarization, and conversational AI. It supports education, research, content creation, and software development while requiring human oversight for critical decisions to ensure accuracy and ethical use.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 2,
    "bes_tracking_id": 2,
    "issue_url": "",
    "type": "LLM",
    "name": "llama3:8b",
    "organization": "Meta AI",
    "description": "Meta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both 8B and 70B parameter sizes (pre-trained or instruction-tuned). Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks.",
    "created_date": "2025-03-18",
    "model_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/meta-llama/llama3",
    "model_card": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    "modality": "text; text",
    "analysis": "",
    "size": "8B Parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Security Benchmark"],
    "access": "open",
    "license": "LLAMA 3 COMMUNITY LICENSE",
    "intended_uses": "Dialogue use cases and chat",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 3,
    "bes_tracking_id": 3,
    "issue_url": "",
    "type": "LLM",
    "name": "llama3.1:8b",
    "organization": "Meta AI",
    "description": "Meta Llama 3.1 is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. The upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta’s latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. Meta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models.",
    "created_date": "2025-03-25",
    "model_url": "https://huggingface.co/meta-llama/Llama-3.1-8B",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/meta-llama/Llama-3.1-8B",
    "model_card": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md",
    "modality": "text; text",
    "analysis": "",
    "size": "8B Params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Security Benchmark"],
    "access": "open",
    "license": "LLAMA 3.1 COMMUNITY LICENSE",
    "intended_uses": "Dialogue use cases and chat",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 4,
    "bes_tracking_id": 4,
    "issue_url": "",
    "type": "LLM",
    "name": "granite-code:8b",
    "organization": "IBM",
    "description": " Granite-code 8b 128K is a decoder-only code model designed for code generative tasks (e.g., code generation, code explanation, code fixing, etc.). It is trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 4 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the models’ ability to reason and follow instructions.",
    "created_date": "2024-10-07",
    "model_url": "https://huggingface.co/ibm-granite/granite-8b-code-base-128k",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/ibm-granite/granite-code-models",
    "model_card": "https://huggingface.co/ibm-granite/granite-8b-code-base-128k",
    "modality": "text; text",
    "analysis": "",
    "size": "8B Params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Security Benchmark"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "Prominent enterprise use cases of LLMs in software engineering productivity include code generation, code explanation, code fixing, generating unit tests, generating documentation, addressing technical debt issues, vulnerability detection, code translation, and more. All Granite Code Base models, including the 3B parameter model, are able to handle these tasks as they were trained on a large amount of code data from 116 programming languages.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 5,
    "bes_tracking_id": 5,
    "issue_url": "",
    "type": "LLM",
    "name": "codellama:7b",
    "organization": "Meta AI",
    "description": "Code Llama is a series of pretrained and fine-tuned generative text models, with sizes ranging from 7 billion to 34 billion parameters. This repository specifically features the 7B instruct-tuned version, optimized for general code synthesis and understanding. The model supports capabilities like code completion, infilling, and instruction following, making it suitable for various programming tasks, particularly in Python.",
    "created_date": "2025-04-01",
    "model_url": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "modality": "text; text",
    "analysis": "",
    "size": "7B Params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "LLAMA 2 COMMUNITY LICENSE",
    "intended_uses": "Intended Use Cases Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 6,
    "bes_tracking_id": 6,
    "issue_url": "",
    "type": "LLM",
    "name": "granite3.2:8b",
    "organization": "IBM",
    "description": "Granite-3.2 is a family of long-context AI models fine-tuned for thinking capabilities. Built on top of Granite-3.1, it has been trained using a mix of permissively licensed open-source datasets and internally generated synthetic data designed for reasoning tasks. The models allow controllability of its thinking capability, ensuring it is applied only when required.",
    "created_date": "2025-03-19",
    "model_url": "https://ollama.com/library/granite3.2",
    "data_url": "",
    "label_url": "",
    "url": "https://ollama.com/library/granite3.2",
    "model_card": "https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision",
    "modality": "text; text",
    "analysis": "",
    "size": "8B Parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "These models are designed to handle general instruction-following tasks and can be integrated into AI assistants across various domains, including business applications",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 7,
    "bes_tracking_id": 7,
    "issue_url": "",
    "type": "SLM",
    "name": "phi4:14b",
    "organization": "Microsoft",
    "description": "Phi-4, 14B parameter state-of-the-art small language model (SLM) that excels at complex reasoning in areas such as math, in addition to conventional language processing. Phi-4 is the latest member of Phi family of small language models and demonstrates what’s possible as we continue to probe the boundaries of SLMs. Phi-4 is available on Azure AI Foundry and on Hugging Face.",
    "created_date": "2025-04-07",
    "model_url": "https://github.com/microsoft/PhiCookBook",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/microsoft/PhiCookBook",
    "model_card": "https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4",
    "modality": "text; text",
    "analysis": "",
    "size": "14B Params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "The model is designed to accelerate research on language models, for use as a building block for generative AI powered features. It provides uses for general purpose AI systems and applications (primarily in English) which require memory/compute constrained environments, latency bound scenarios, reasoning and logic.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 8,
    "bes_tracking_id": 8,
    "issue_url": "",
    "type": "LLM",
    "name": "mistral:7b",
    "organization": "Mistral AI",
    "description": "Mistral 7B is a pre-trained generative text model with 7 billion parameters. Mistral 7B is a transformed models with the architecture choices as Grouped-Query Attention, Sliding Window Attention, Byte-fallback BPE tokenizer.",
    "created_date": "2025-04-07",
    "model_url": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/mistralai",
    "model_card": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
    "modality": "text; text",
    "analysis": "",
    "size": "7B Params",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "Chat",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 9,
    "bes_tracking_id": 9,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-base-openai-detector",
    "organization": "openai-community",
    "description": "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the largest GPT-2 model, the 1.5B parameter version.",
    "created_date": "2024-09-1",
    "model_url": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/openai/gpt-2-output-dataset/tree/master/detector",
    "model_card": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "The model is a classifier that can be used to detect text generated by GPT-2 models. However, it is strongly suggested not to use it as a ChatGPT detector for the purposes of making grave allegations of academic misconduct against undergraduates and others, as this model might give inaccurate results in the case of ChatGPT-generated input.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/openai-community/roberta-base-openai-detector/discussions"
  },
  {
    "id": 10,
    "bes_tracking_id": 10,
    "issue_url": "",
    "type": "Classic",
    "name": "bes_image_classification",
    "organization": "Be-Secure",
    "description": "This repository provides reference implementations of a machine learning model specifically designed for image classification tasks, focusing on the recognition of handwritten digits using the MNIST dataset. The MNIST dataset, a classic benchmark in the field of computer vision, comprises 70,000 grayscale images of handwritten digits (0-9), with 60,000 images dedicated to training and 10,000 for testing. This dataset serves as a foundational resource for developing and evaluating image classification algorithms.The implementation utilizes TensorFlow Datasets, ensuring a seamless and efficient way to load and preprocess the MNIST data. The model architecture is built upon modern deep learning techniques, commonly employing convolutional neural networks (CNNs) to effectively capture the spatial hierarchies in the image data. By leveraging convolutional layers, pooling layers, and activation functions, the model is able to learn robust features that significantly enhance its classification performance.",
    "created_date": "2023-11-30",
    "model_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/model.zip",
    "data_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/data.zip",
    "label_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/label.zip",
    "url": "https://github.com/Be-Secure/bes_image_classification",
    "model_card": "https://github.com/Be-Secure/bes_image_classification",
    "modality": "image; image",
    "analysis": "",
    "size": "5.6 MB",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST", "Fuzz Test"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 11,
    "bes_tracking_id": 11,
    "issue_url": "",
    "type": "Classic",
    "name": "structured-data-classification-grn-vsn",
    "organization": "keras-io",
    "description": "The model leverages two key components from Bryan Lim et al.'s Temporal Fusion Transformers (TFT) framework for time series forecasting: Gated Residual Networks (GRN) and Variable Selection Networks (VSN). These components are effective for structured data learning tasks. Gated Residual Networks (GRN) enhance information flow through skip connections and gating layers, enabling selective non-linear processing. The GRN processes inputs by applying a non-linear ELU transformation, followed by a linear transformation with dropout. It then utilizes Gated Linear Units (GLUs) to filter out irrelevant inputs before adding the original inputs back to the output for residual connections, and finally normalizes the output. Variable Selection Networks (VSN) focus on identifying the most important features while eliminating noisy inputs that could degrade performance. Each feature is first processed through a GRN, then concatenated and subjected to another GRN. This is followed by a softmax function to generate feature weights, leading to a weighted sum of the outputs from the individual GRNs.",
    "created_date": "2022-07-04",
    "model_url": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for binary classification task to determine whether a person makes over $500K a year or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn/discussions"
  },
  {
    "id": 12,
    "bes_tracking_id": 12,
    "issue_url": "",
    "type": "Classic",
    "name": "xgboost-example",
    "organization": "scikit-learn",
    "description": "This repository showcases an XGBoost model specifically developed to predict the daily alcohol consumption of students, utilizing the powerful capabilities of the XGBoost framework in conjunction with the Scikit-learn library. The model is designed to analyze various factors that may influence students' drinking habits, leveraging both demographic and behavioral data to provide accurate predictions.The dataset used for training the model includes a comprehensive collection of features such as age, gender, academic performance, social interactions, and lifestyle choices, which are all critical indicators of alcohol consumption patterns. By employing XGBoost, a gradient boosting framework known for its speed and accuracy, the model is able to capture complex relationships within the data, enhancing its predictive performance.",
    "created_date": "2023-01-20",
    "model_url": "https://huggingface.co/scikit-learn/xgboost-example",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/scikit-learn/xgboost-example",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 13,
    "bes_tracking_id": 13,
    "issue_url": "",
    "type": "Classic",
    "name": "TF_Decision_Trees",
    "organization": "keras-io",
    "description": "Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is implemented using Tensorflow 7.0 or higher. The US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables was used to train it. This is a binary classification problem to determine whether a person makes over 50k a year.",
    "created_date": "2022-02-15",
    "model_url": "https://huggingface.co/keras-io/TF_Decision_Trees",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/TF_Decision_Trees",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/TF_Decision_Trees/discussions"
  },
  {
    "id": 14,
    "bes_tracking_id": 14,
    "issue_url": "",
    "type": "Classic",
    "name": "tab_transformer",
    "organization": "keras-io",
    "description": "The TabTransformer model in Keras, from keras-io, is designed specifically for tabular data, which is structured in rows and columns like a spreadsheet or a relational database. Traditional deep learning models struggle with tabular data since it lacks spatial or temporal structure, unlike image or sequential data. However, the TabTransformer leverages transformer architectures, which excel at capturing relationships between input features, to model dependencies and interactions within tabular data.",
    "created_date": "2022-06-11",
    "model_url": "https://huggingface.co/keras-io/tab_transformer",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/tab_transformer",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/tab_transformer/discussions"
  },
  {
    "id": 15,
    "bes_tracking_id": 15,
    "issue_url": "",
    "type": "Classic",
    "name": "imbalanced_classification",
    "organization": "keras-io",
    "description": "This repository contains a deep learning model implemented in Keras for imbalanced classification, specifically focused on detecting fraudulent credit card transactions. The model is trained to handle a highly imbalanced dataset, where fraudulent transactions represent a small fraction of all transactions. By applying class weights during training, the model aims to minimize false negatives, thus improving the reliability of fraud detection.",
    "created_date": "2022-06-13",
    "model_url": "https://huggingface.co/keras-io/imbalanced_classification",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/imbalanced_classification",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "The trained model is used to detect of a specific transaction is fraudulent or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/imbalanced_classification/discussions"
  },
  {
    "id": 16,
    "bes_tracking_id": 16,
    "issue_url": "",
    "type": "Classic",
    "name": "breast_cancernb8gjv4n-diagnosis-classification",
    "organization": "merve",
    "description": "The Hugging Face model merve/breast_cancernb8gjv4n-diagnosis-classification is a baseline diagnostic classification tool for breast cancer. It uses a Logistic Regression model trained on the breast_cancernb8gjv4n dataset, achieving high performance with metrics such as 97.89% accuracy, 99.43% average precision, and 99.54% ROC AUC. The model is designed with balanced class weights and uses EasyPreprocessor for feature processing, emphasizing robust baseline results.",
    "created_date": "2022-07-06",
    "model_url": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification/discussions"
  },
  {
    "id": 17,
    "bes_tracking_id": 17,
    "issue_url": "",
    "type": "Classic",
    "name": "tabular-playground",
    "organization": "demo-org",
    "description": "DecisionTreeClassifier model trained on Kaggle's August 2022 Tabular Playground Series dataset, specifically designed to handle complex tabular data for classification tasks. The model implements various preprocessing techniques to ensure optimal performance, including imputation of missing values and one-hot encoding of categorical features. This preprocessing step is crucial for preparing the dataset, allowing the model to effectively learn from the available information without being hindered by incomplete data.The DecisionTreeClassifier is configured with a maximum depth of 4, which helps to prevent overfitting while still capturing the essential patterns within the data. This depth setting strikes a balance between model complexity and interpretability, making it easier to visualize and understand the decision-making process behind the predictions.",
    "created_date": "2022-08-30",
    "model_url": "https://huggingface.co/demo-org/tabular-playground",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/demo-org/tabular-playground",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/demo-org/tabular-playground/discussions"
  },
  {
    "id": 18,
    "bes_tracking_id": 18,
    "issue_url": "",
    "type": "Classic",
    "name": "emb-gam-dino",
    "organization": "Ramos-Ramos",
    "description": "This is a LogisticRegressionCV classifier based on patch embeddings from the Imagenette dataset. Using embeddings from the DINO model, it implements Generalized Additive Models (GAM) for efficient and interpretable image classification. The model achieves high accuracy (97.7%) and offers insights into patch-level contributions, ideal for transparent AI applications.",
    "created_date": "2022-10-30",
    "model_url": "https://huggingface.co/Ramos-Ramos/emb-gam-dino",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/Ramos-Ramos/emb-gam-dino",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/Ramos-Ramos/emb-gam-dino/discussions"
  },
  {
    "id": 19,
    "bes_tracking_id": 19,
    "issue_url": "",
    "type": "Classic",
    "name": "ml-generation-failure-prediction",
    "organization": "moro23",
    "description": "This repository contains an XGBoost-based classifier specifically developed to predict failures in generators, utilizing binary logistic regression techniques for precise failure detection. The model is strategically designed to address the challenges of imbalanced data, which is common in failure prediction scenarios where instances of failure are significantly less frequent than normal operations.To enhance its predictive capabilities, the classifier is trained with carefully balanced parameters that optimize performance and accuracy. Key hyperparameters include a learning rate of 0.1, which controls the step size at each iteration during training to ensure effective learning without overshooting the optimal solution. The model features a maximum depth of 6, which allows it to capture complex interactions and relationships in the data while preventing overfitting by maintaining a manageable tree complexity.",
    "created_date": "2022-12-17",
    "model_url": "https://huggingface.co/moro23/ml-generation-failure-prediction",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/moro23/ml-generation-failure-prediction",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/moro23/ml-generation-failure-prediction/discussions"
  },
  {
    "id": 20,
    "bes_tracking_id": 20,
    "issue_url": "",
    "type": "Classic",
    "name": "Image_Segmentation",
    "organization": "Be-Secure",
    "description": "This repository features an image classification model developed by the Be-Secure community, specifically designed to recognize handwritten digits from the MNIST dataset. The MNIST dataset is a widely used benchmark in the field of machine learning, consisting of 70,000 images of handwritten digits (0-9), with 60,000 training images and 10,000 testing images. This dataset provides a rich resource for training and evaluating image classification algorithms, enabling the development of robust models for digit recognition tasks.The model employs advanced techniques in deep learning, utilizing convolutional neural networks (CNNs) to achieve high accuracy in classifying the images. By leveraging the hierarchical feature extraction capabilities of CNNs, the model effectively captures the intricate patterns and characteristics of handwritten digits, resulting in improved performance over traditional methods.",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST", "Fuzz Test"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 21,
    "bes_tracking_id": 21,
    "issue_url": "",
    "type": "Classic",
    "name": "Timeseries_Forecasting",
    "organization": "Be-Secure",
    "description": "Reference implementation of both an XGBoost Regressor and a Long Short-Term Memory (LSTM) model, specifically designed for time series forecasting tasks. The implementation focuses on two distinct datasets: the House Energy Consumption dataset, which can be accessed through the Scikit-learn framework, and the Pump Sensor dataset, which is utilized within a TensorFlow model.The House Energy Consumption dataset contains detailed records of energy usage across different households, including features such as timestamp, energy consumption, temperature, and humidity. This dataset allows users to explore and analyze the patterns of energy usage over time, making it an excellent resource for forecasting future energy demands. The XGBoost Regressor, known for its efficiency and effectiveness in handling tabular data, is employed to predict energy consumption based on historical data trends.",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST", "Fuzz Test"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 22,
    "bes_tracking_id": 22,
    "issue_url": "",
    "type": "Classic",
    "name": "Tabular_Classification",
    "organization": "Be-Secure",
    "description": "Reference implementation of an XGBoost model specifically designed for tabular classification tasks, with a focus on analyzing banking marketing campaign data. The implementation utilizes the widely recognized banking marketing campaign dataset, which is available for download from the UCI Machine Learning Repository at UCI Machine Learning Repository. In this implementation, we leverage the powerful capabilities of XGBoost, an optimized gradient boosting framework known for its performance and efficiency in handling large datasets with complex relationships. The model is designed to facilitate the classification of clients into two categories: those who are likely to subscribe to the term deposit and those who are not.",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "Not Applicable",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST", "Fuzz Test"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 23,
    "bes_tracking_id": 23,
    "issue_url": "",
    "type": "LLM",
    "name": "MathCoder",
    "organization": "Shanghai AI Laboratory",
    "description": "MathCoder is a state-of-the-art generative model designed for automatic mathematical problem-solving and content generation. Built on a transformer architecture, MathCoder is adept at understanding and manipulating mathematical expressions, equations, and problems presented in natural language. The model is trained on a comprehensive dataset that includes a wide variety of mathematical concepts and problem types, enabling it to accurately interpret user queries and produce relevant solutions.With its ability to tackle diverse mathematical tasks, MathCoder can perform operations such as equation simplification, algebraic problem-solving, calculus computations, and even generate detailed step-by-step solutions. This makes it an essential tool for students, educators, and professionals seeking to enhance their mathematical understanding and capabilities.",
    "created_date": "2023-10-05",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": ["GPT-4", "LLaMA 2"],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 24,
    "bes_tracking_id": 24,
    "issue_url": "",
    "type": "LLM",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is an advanced open-access language model developed by TII, boasting 180 billion parameters and trained on a vast dataset of 3,500 billion tokens from RefinedWeb, supplemented with curated corpora. It is designed primarily for text generation and other language processing tasks, showcasing superior performance compared to several existing models like LLaMA-2 and StableLM.",
    "created_date": "2023-09-06",
    "model_url": "https://huggingface.co/tiiuae/falcon-180B",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": ["RefinedWeb"],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "id": 25,
    "bes_tracking_id": 25,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-18",
    "organization": "microsoft",
    "description": "ResNet-18, developed by Microsoft, is a convolutional neural network architecture that incorporates residual connections, allowing for effective training of deep networks. With 18 layers, it excels in image classification tasks and won the 2015 ILSVRC & COCO competition. The model can classify images into one of 1,000 ImageNet classes and is available for further fine-tuning on specific tasks. Users can easily implement it using the Hugging Face Transformers library.",
    "created_date": "2022-06-07",
    "model_url": "https://huggingface.co/microsoft/resnet-18",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-18",
    "modality": "",
    "analysis": "",
    "size": "11.7M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 26,
    "bes_tracking_id": 26,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-arabic-camelbert-ca-pos-glf",
    "organization": "CAMeL-Lab",
    "description": "The CAMeLBERT-CA POS-GLF model is a Gulf Arabic part-of-speech (POS) tagging model fine-tuned from the CAMeLBERT-CA architecture using the Gumar dataset. Designed for Arabic language processing, this model can effectively identify and classify tokens within text, making it suitable for various natural language processing tasks. Users can easily implement it through the Hugging Face Transformers library, leveraging its capabilities for token classification in Arabic texts. The model's training details and fine-tuning methodology are documented in the related academic paper.",
    "created_date": "2021-10-18",
    "model_url": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 27,
    "bes_tracking_id": 27,
    "issue_url": "",
    "type": "LLM",
    "name": "all-MiniLM-L6-v2",
    "organization": "sentence-transformers",
    "description": "The all-MiniLM-L6-v2 model is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, making it suitable for tasks such as semantic search and clustering. It is based on the pretrained MiniLM-L6 architecture and has been fine-tuned on a large dataset of over 1 billion sentence pairs using a contrastive learning approach. Users can easily implement the model using the Sentence-Transformers library or the Hugging Face Transformers library for various natural language processing applications, including sentence similarity and information retrieval.",
    "created_date": "2021-08-31",
    "model_url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "1B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "as a sentence and short paragraph encoder",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 28,
    "bes_tracking_id": 28,
    "issue_url": "",
    "type": "LLM",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "organization": "sentence-transformers",
    "description": "The paraphrase-multilingual-MiniLM-L12-v2 model is a sentence-transformers model designed to map sentences and paragraphs into a 384-dimensional dense vector space, making it suitable for tasks like semantic search and clustering. This model is particularly adept at handling multiple languages, providing embeddings that capture the semantic information of the input text.",
    "created_date": "2021-06-09",
    "model_url": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 29,
    "bes_tracking_id": 29,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-NER",
    "organization": "dslim",
    "description": "bert-base-NER is a specialized BERT model that has been fine-tuned specifically for the Named Entity Recognition (NER) task, showcasing state-of-the-art performance in identifying and classifying named entities in text. This model is adept at recognizing four distinct types of entities: locations (LOC), organizations (ORG), people (PER), and miscellaneous items (MISC). By leveraging the powerful transformer architecture of BERT, it captures contextual information effectively, leading to accurate entity recognition.",
    "created_date": "2020-12-12",
    "model_url": "https://huggingface.co/dslim/bert-base-NER",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/dslim/bert-base-NER",
    "modality": "",
    "analysis": "",
    "size": "108M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 30,
    "bes_tracking_id": 30,
    "issue_url": "",
    "type": "LLM",
    "name": "vit-base-patch16-224",
    "organization": "google",
    "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch.",
    "created_date": "2021-03-24",
    "model_url": "https://huggingface.co/google/vit-base-patch16-224",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/google/vit-base-patch16-224",
    "modality": "image; image",
    "analysis": "",
    "size": "86.6M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 31,
    "bes_tracking_id": 31,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-50",
    "organization": "microsoft",
    "description": "ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models. This is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.",
    "created_date": "2022-06-07",
    "model_url": "https://huggingface.co/microsoft/resnet-50",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-50",
    "modality": "image; image",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 32,
    "bes_tracking_id": 32,
    "issue_url": "",
    "type": "LLM",
    "name": "collaborative-filtering-movielens",
    "organization": "keras-io",
    "description": "The Collaborative Filtering model implemented in this repository is designed to provide personalized movie recommendations by leveraging user-item interaction data. At its core, the model utilizes a neural network architecture to learn the underlying patterns in user preferences and item characteristics based on historical ratings or interactions.By employing Collaborative Filtering techniques, the model can identify relationships between users and movies, even in cases where there are few explicit ratings. It achieves this by analyzing similarities among users and movies, allowing it to recommend movies that similar users have enjoyed, thereby enhancing the personalization of the recommendations.The model processes user and item data through an embedding layer, which transforms categorical variables into dense vector representations. These embeddings capture latent features of both users and movies, enabling the model to understand complex relationships in the data. The architecture may also incorporate additional layers, such as fully connected layers, to refine the learned representations and improve the accuracy of the recommendations.",
    "created_date": "2022-06-02",
    "model_url": "https://huggingface.co/keras-io/collaborative-filtering-movielens",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/collaborative-filtering-movielens",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "CC0 1.0",
    "intended_uses": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/collaborative-filtering-movielens/discussions"
  },
  {
    "id": 33,
    "bes_tracking_id": 33,
    "issue_url": "",
    "type": "Classic",
    "name": "modeltest",
    "organization": "vendorabc",
    "description": "This model is a HistGradientBoostingClassifier, specifically trained on the well-known Breast Cancer dataset to predict cancer diagnoses based on clinical features. To optimize its performance, the model was trained using Halving Grid Search Cross Validation, an efficient hyperparameter tuning method that iteratively narrows down the parameter search space. The search focused on tuning max_leaf_nodes and max_depth, two critical parameters that influence the complexity and accuracy of the decision trees within the gradient boosting ensemble. By leveraging these parameters, the model achieves a robust balance between interpretability and predictive power, making it suitable for reliable medical predictions.",
    "created_date": "2022-08-30",
    "model_url": "https://huggingface.co/vendorabc/modeltest",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/vendorabc/modeltest",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["SAST"],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/vendorabc/modeltest/discussions"
  },
  {
    "id": 34,
    "bes_tracking_id": 34,
    "issue_url": "",
    "type": "LLM",
    "name": "flan-t5-xxl",
    "organization": "google",
    "description": "FLAN-T5-XXL is an advanced language model that improves upon the original T5 architecture by being fine-tuned on over 1,000 additional tasks across multiple languages. This model excels in various natural language processing applications, achieving state-of-the-art performance in tasks such as question answering and text generation, particularly in few-shot and zero-shot settings. Its design allows for better usability and effectiveness in a range of NLP tasks.",
    "created_date": "2024-09-1",
    "model_url": "https://huggingface.co/google/flan-t5-xxl",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/google-research/t5x",
    "model_card": "https://huggingface.co/google/flan-t5-xxl",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The model is designed for tasks where only a few or no examples are provided during fine-tuning, such as reasoning and question-answering tasks.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google/flan-t5-xxl/discussions"
  },
  {
    "id": 35,
    "bes_tracking_id": 35,
    "issue_url": "",
    "type": "LLM",
    "name": "distilbert-base-uncased-finetuned-sst-2-english",
    "organization": "distilbert",
    "description": "DistilBERT Base Uncased Fine-tuned on SST-2 is a compact yet powerful text classification model designed to perform sentiment analysis. This fine-tuned version of DistilBERT achieves an impressive accuracy of 91.3% on the Stanford Sentiment Treebank (SST-2) dataset, making it a reliable choice for evaluating sentiment in English texts. With its efficient architecture, it provides a balance between performance and computational efficiency, enabling effective sentiment classification for various applications.",
    "created_date": "2024-09-1",
    "model_url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "This model can be used for topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/discussions"
  },
  {
    "id": 36,
    "bes_tracking_id": 36,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-uncased",
    "organization": "google-bert",
    "description": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives: Masked language modeling (MLM) and Next sentence prediction (NSP). This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard classifier using the features produced by the BERT model as inputs.",
    "created_date": "2024-09-1",
    "model_url": "https://huggingface.co/google-bert/bert-base-uncased",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/google-research/bert/blob/master/README.md",
    "model_card": "https://huggingface.co/google-bert/bert-base-uncased",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions of a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google-bert/bert-base-uncased/discussions"
  },
  {
    "id": 37,
    "bes_tracking_id": 37,
    "issue_url": "",
    "type": "LLM",
    "name": "codet5-base",
    "organization": "Salesforce",
    "description": "CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Salesforce/codet5-base",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/Salesforce/codet5-base",
    "model_card": "https://huggingface.co/Salesforce/codet5-base",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 38,
    "bes_tracking_id": 38,
    "issue_url": "",
    "type": "LLM",
    "name": "gpt-neo-1.3B",
    "organization": "EleutherAI",
    "description": "GPT-Neo 1.3B is a transformer model developed by EleutherAI, featuring 1.3 billion parameters and based on the GPT-3 architecture. Trained on the diverse Pile dataset, it excels at text generation and predictive tasks. While capable of generating coherent and contextually relevant text, users should be mindful of potential biases and the risk of producing socially unacceptable content.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/EleutherAI/gpt-neo",
    "model_card": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "modality": "text; text",
    "analysis": "",
    "size": "1.37B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 39,
    "bes_tracking_id": 39,
    "issue_url": "",
    "type": "LLM",
    "name": "detoxify",
    "organization": "unitaryai",
    "description": "Detoxify is a library designed to predict toxic comments using state-of-the-art models trained on multiple Jigsaw challenges. It provides three models: an original model for general toxicity detection, an unbiased model aimed at minimizing identity bias, and a multilingual model capable of classifying comments in seven languages. With recent updates, Detoxify ensures consistent class naming and improved performance metrics, making it a valuable tool for content moderation and research in identifying harmful online content.",
    "created_date": "2024-10-01",
    "model_url": "https://github.com/unitaryai/detoxify",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/unitaryai/detoxify",
    "model_card": "https://github.com/unitaryai/detoxify",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics and/or to aid content moderators in flagging out harmful content quicker.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 40,
    "bes_tracking_id": 8,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-cwe-classifier-kelemia",
    "organization": "Dunateo",
    "description": "Kelemia for CWE Classification is a specialized model based on RoBERTa, fine-tuned for classifying software vulnerabilities according to the Common Weakness Enumeration (CWE) standard. This model serves as a critical tool in the security analysis process, achieving robust performance through careful training on relevant datasets. While it offers valuable insights into potential vulnerabilities, it should be utilized as part of a comprehensive security framework rather than a standalone solution, as it may produce false positives or negatives that require validation by security professionals.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "modality": "text; text",
    "analysis": "",
    "size": "125M",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is intended for classifying software vulnerabilities according to the CWE standard. It should be used as part of a broader security analysis process and not as a standalone solution for identifying vulnerabilities.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 41,
    "bes_tracking_id": 9,
    "issue_url": "",
    "type": "LLM",
    "name": "deberta-v3-base-prompt-injection-v2",
    "organization": "protectai",
    "description": "This model is a fine-tuned version of microsoft/deberta-v3-base specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs. Prompt injection attacks manipulate language models by inserting or altering prompts to trigger harmful or unintended responses. The deberta-v3-base-prompt-injection-v2 model is designed to enhance security in language model applications by detecting these malicious interventions.",
    "created_date": "2024-09-1",
    "model_url": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2/discussions"
  }
]
