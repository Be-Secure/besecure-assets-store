[
  {
    "id": 1,
    "bes_tracking_id": 1,
    "issue_url": "",
    "type": "Classic",
    "name": "bes_image_classification",
    "organization": "Be-Secure",
    "description": "Reference implementations of an MNIST model for image classification, specifically focusing on handwritten digit data. The implementation utilizes the MNIST dataset, which is part of TensorFlow Datasets",
    "created_date": "2023-11-30",
    "model_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/model.zip",
    "data_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/data.zip",
    "label_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/label.zip",
    "url": "https://github.com/Be-Secure/bes_image_classification",
    "model_card": "https://github.com/Be-Secure/bes_image_classification",
    "modality": "image; image",
    "analysis": "",
    "size": "5.6 MB",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 2,
    "bes_tracking_id": 2,
    "issue_url": "",
    "type": "Classic",
    "name": "structured-data-classification-grn-vsn",
    "organization": "keras-io",
    "description": "This model is built using two important architectural components proposed by Bryan Lim et al. in Temporal Fusion Transformers (TFT) for Interpretable Multi-horizon Time Series Forecasting called GRN and VSN which are very useful for structured data learning tasks.",
    "created_date": "2022-07-04",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for binary classification task to determine whether a person makes over $500K a year or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn/discussions"
  },
  {
    "id": 3,
    "bes_tracking_id": 3,
    "issue_url": "",
    "type": "Classic",
    "name": "xgboost-example",
    "organization": "scikit-learn",
    "description": "This is an XGBoost model trained to predict daily alcohol consumption of students.",
    "created_date": "2023-01-20",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/scikit-learn/xgboost-example",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 4,
    "bes_tracking_id": 4,
    "issue_url": "",
    "type": "Classic",
    "name": "TF_Decision_Trees",
    "organization": "keras-io",
    "description": "Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model. The model is implemented using Tensorflow 7.0 or higher. The US Census Income Dataset containing approximately 300k instances with 41 numerical and categorical variables was used to train it. This is a binary classification problem to determine whether a person makes over 50k a year.",
    "created_date": "2022-02-15",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/TF_Decision_Trees",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/TF_Decision_Trees/discussions"
  },
  {
    "id": 5,
    "bes_tracking_id": 5,
    "issue_url": "",
    "type": "Classic",
    "name": "tab_transformer",
    "organization": "keras-io",
    "description": "The TabTransformer model in Keras, from keras-io, is designed specifically for tabular data, which is structured in rows and columns like a spreadsheet or a relational database. Traditional deep learning models struggle with tabular data since it lacks spatial or temporal structure, unlike image or sequential data. However, the TabTransformer leverages transformer architectures, which excel at capturing relationships between input features, to model dependencies and interactions within tabular data.",
    "created_date": "2022-06-11",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/tab_transformer",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/tab_transformer/discussions"
  },
  {
    "id": 6,
    "bes_tracking_id": 6,
    "issue_url": "",
    "type": "Classic",
    "name": "imbalanced_classification",
    "organization": "keras-io",
    "description": "This repository contains a deep learning model implemented in Keras for imbalanced classification, specifically focused on detecting fraudulent credit card transactions. The model is trained to handle a highly imbalanced dataset, where fraudulent transactions represent a small fraction of all transactions. By applying class weights during training, the model aims to minimize false negatives, thus improving the reliability of fraud detection.",
    "created_date": "2022-06-13",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/imbalanced_classification",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "The trained model is used to detect of a specific transaction is fraudulent or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/imbalanced_classification/discussions"
  },
  {
    "id": 7,
    "bes_tracking_id": 7,
    "issue_url": "",
    "type": "Classic",
    "name": "breast_cancernb8gjv4n-diagnosis-classification",
    "organization": "merve",
    "description": "The Hugging Face model merve/breast_cancernb8gjv4n-diagnosis-classification is a baseline diagnostic classification tool for breast cancer. It uses a Logistic Regression model trained on the breast_cancernb8gjv4n dataset, achieving high performance with metrics such as 97.89% accuracy, 99.43% average precision, and 99.54% ROC AUC. The model is designed with balanced class weights and uses EasyPreprocessor for feature processing, emphasizing robust baseline results.",
    "created_date": "2022-07-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification/discussions"
  },
  {
    "id": 8,
    "bes_tracking_id": 8,
    "issue_url": "",
    "type": "Classic",
    "name": "tabular-playground",
    "organization": "demo-org",
    "description": "This is a DecisionTreeClassifier model trained on Kaggle's August 2022 Tabular Playground Series dataset. This classifier handles tabular data with imputed missing values, one-hot encoding, and a model depth of 4. Evaluation metrics include accuracy and F1 score. Model setup guidance is available for users to start quickly.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/demo-org/tabular-playground",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/demo-org/tabular-playground/discussions"
  },
  {
    "id": 9,
    "bes_tracking_id": 9,
    "issue_url": "",
    "type": "Classic",
    "name": "emb-gam-dino",
    "organization": "Ramos-Ramos",
    "description": "This is a LogisticRegressionCV classifier based on patch embeddings from the Imagenette dataset. Using embeddings from the DINO model, it implements Generalized Additive Models (GAM) for efficient and interpretable image classification. The model achieves high accuracy (97.7%) and offers insights into patch-level contributions, ideal for transparent AI applications.",
    "created_date": "2022-10-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/Ramos-Ramos/emb-gam-dino",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/Ramos-Ramos/emb-gam-dino/discussions"
  },
  {
    "id": 10,
    "bes_tracking_id": 10,
    "issue_url": "",
    "type": "Classic",
    "name": "ml-generation-failure-prediction",
    "organization": "moro23",
    "description": "This is a XGBoost-based classifier designed to predict failures in generators, using binary logistic regression. Trained with balanced parameters for accurate failure prediction, it has specific settings such as a learning rate of 0.1, max depth of 6, and gamma value of 3 to handle imbalanced data.",
    "created_date": "2022-12-17",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/moro23/ml-generation-failure-prediction",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/moro23/ml-generation-failure-prediction/discussions"
  },
  {
    "id": 11,
    "bes_tracking_id": 11,
    "issue_url": "",
    "type": "Classic",
    "name": "Image_Segmentation",
    "organization": "Be-Secure",
    "description": "Image classification model developed by Be-Secure community. This model trained using the MNIST dataset",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 12,
    "bes_tracking_id": 12,
    "issue_url": "",
    "type": "Classic",
    "name": "Timeseries_Forecasting",
    "organization": "Be-Secure",
    "description": "Reference implementation of an XGBoost Regressor / LSTM model for time series forecasting. The implementation focuses on the House Energy Consumption dataset (Sci-kit learn framework) and the Pump Sensor dataset (TensorFlow model).",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 13,
    "bes_tracking_id": 13,
    "issue_url": "",
    "type": "Classic",
    "name": "Tabular_Classification",
    "organization": "Be-Secure",
    "description": "Reference implementation of an XGBoost model for tabular classification, focused on banking marketing campaign data.The implementation utilizes the banking marketing campaign dataset, which can be downloaded from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/bank+marketing",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "Not Applicable",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 14,
    "bes_tracking_id": 14,
    "issue_url": "",
    "type": "LLM",
    "name": "MathCoder",
    "organization": "Shanghai AI Laboratory",
    "description": "MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.",
    "created_date": "2023-10-05",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": [
      "GPT-4",
      "LLaMA 2"
    ],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 15,
    "bes_tracking_id": 15,
    "issue_url": "",
    "type": "LLM",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is an advanced open-access language model developed by TII, boasting 180 billion parameters and trained on a vast dataset of 3,500 billion tokens from RefinedWeb, supplemented with curated corpora. It is designed primarily for text generation and other language processing tasks, showcasing superior performance compared to several existing models like LLaMA-2 and StableLM.",
    "created_date": "2023-09-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://falconllm.tii.ae/falcon-models.html",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": [
      "RefinedWeb"
    ],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "id": 16,
    "bes_tracking_id": 16,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-18",
    "organization": "microsoft",
    "description": "ResNet-18, developed by Microsoft, is a convolutional neural network architecture that incorporates residual connections, allowing for effective training of deep networks. With 18 layers, it excels in image classification tasks and won the 2015 ILSVRC & COCO competition. The model can classify images into one of 1,000 ImageNet classes and is available for further fine-tuning on specific tasks. Users can easily implement it using the Hugging Face Transformers library.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-18",
    "modality": "",
    "analysis": "",
    "size": "11.7M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 17,
    "bes_tracking_id": 17,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-arabic-camelbert-ca-pos-glf",
    "organization": "CAMeL-Lab",
    "description": "The CAMeLBERT-CA POS-GLF model is a Gulf Arabic part-of-speech (POS) tagging model fine-tuned from the CAMeLBERT-CA architecture using the Gumar dataset. Designed for Arabic language processing, this model can effectively identify and classify tokens within text, making it suitable for various natural language processing tasks. Users can easily implement it through the Hugging Face Transformers library, leveraging its capabilities for token classification in Arabic texts. The model's training details and fine-tuning methodology are documented in the related academic paper.",
    "created_date": "2021-10-18",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 18,
    "bes_tracking_id": 18,
    "issue_url": "",
    "type": "LLM",
    "name": "all-MiniLM-L6-v2",
    "organization": "sentence-transformers",
    "description": "The all-MiniLM-L6-v2 model is a sentence-transformers model that converts sentences and paragraphs into 384-dimensional dense vector representations, making it suitable for tasks such as semantic search and clustering. It is based on the pretrained MiniLM-L6 architecture and has been fine-tuned on a large dataset of over 1 billion sentence pairs using a contrastive learning approach. Users can easily implement the model using the Sentence-Transformers library or the Hugging Face Transformers library for various natural language processing applications, including sentence similarity and information retrieval.",
    "created_date": "2021-08-31",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "1B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "as a sentence and short paragraph encoder",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 19,
    "bes_tracking_id": 19,
    "issue_url": "",
    "type": "LLM",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "organization": "sentence-transformers",
    "description": "The paraphrase-multilingual-MiniLM-L12-v2 model is a sentence-transformers model designed to map sentences and paragraphs into a 384-dimensional dense vector space, making it suitable for tasks like semantic search and clustering. This model is particularly adept at handling multiple languages, providing embeddings that capture the semantic information of the input text.",
    "created_date": "2021-06-09",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 20,
    "bes_tracking_id": 20,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-NER",
    "organization": "dslim",
    "description": "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
    "created_date": "2020-12-12",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/dslim/bert-base-NER",
    "modality": "",
    "analysis": "",
    "size": "108M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 21,
    "bes_tracking_id": 21,
    "issue_url": "",
    "type": "LLM",
    "name": "vit-base-patch16-224",
    "organization": "google",
    "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch.",
    "created_date": "2021-03-24",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/google/vit-base-patch16-224",
    "modality": "image; image",
    "analysis": "",
    "size": "86.6M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 22,
    "bes_tracking_id": 22,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-50",
    "organization": "microsoft",
    "description": "ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models. This is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-50",
    "modality": "image; image",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 23,
    "bes_tracking_id": 23,
    "issue_url": "",
    "type": "LLM",
    "name": "collaborative-filtering-movielens",
    "organization": "keras-io",
    "description": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "created_date": "2022-06-02",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/collaborative-filtering-movielens",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "CC0 1.0",
    "intended_uses": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/collaborative-filtering-movielens/discussions"
  },
  {
    "id": 24,
    "bes_tracking_id": 24,
    "issue_url": "",
    "type": "Classic",
    "name": "modeltest",
    "organization": "vendorabc",
    "description": "This is a HistGradientBoostingClassifier model trained on breast cancer dataset. It's trained with Halving Grid Search Cross Validation, with parameter grids on max_leaf_nodes and max_depth.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/vendorabc/modeltest",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/vendorabc/modeltest/discussions"
  },
  {
    "id": 25,
    "bes_tracking_id": 25,
    "issue_url": "",
    "type": "LLM",
    "name": "deberta-v3-base-prompt-injection-v2",
    "organization": "protectai",
    "description": "This model is a fine-tuned version of microsoft/deberta-v3-base specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs. Prompt injection attacks manipulate language models by inserting or altering prompts to trigger harmful or unintended responses. The deberta-v3-base-prompt-injection-v2 model is designed to enhance security in language model applications by detecting these malicious interventions.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "model_card": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2/discussions"
  },
  {
    "id": 26,
    "bes_tracking_id": 26,
    "issue_url": "",
    "type": "LLM",
    "name": "flan-t5-xxl",
    "organization": "google",
    "description": "FLAN-T5-XXL is an advanced language model that improves upon the original T5 architecture by being fine-tuned on over 1,000 additional tasks across multiple languages. This model excels in various natural language processing applications, achieving state-of-the-art performance in tasks such as question answering and text generation, particularly in few-shot and zero-shot settings. Its design allows for better usability and effectiveness in a range of NLP tasks.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/google/flan-t5-xxl",
    "model_card": "https://huggingface.co/google/flan-t5-xxl",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The model is designed for tasks where only a few or no examples are provided during fine-tuning, such as reasoning and question-answering tasks.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google/flan-t5-xxl/discussions"
  },
  {
    "id": 27,
    "bes_tracking_id": 27,
    "issue_url": "",
    "type": "LLM",
    "name": "distilbert-base-uncased-finetuned-sst-2-english",
    "organization": "distilbert",
    "description": "DistilBERT Base Uncased Fine-tuned on SST-2 is a compact yet powerful text classification model designed to perform sentiment analysis. This fine-tuned version of DistilBERT achieves an impressive accuracy of 91.3% on the Stanford Sentiment Treebank (SST-2) dataset, making it a reliable choice for evaluating sentiment in English texts. With its efficient architecture, it provides a balance between performance and computational efficiency, enabling effective sentiment classification for various applications.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "model_card": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "This model can be used for topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/discussions"
  },
  {
    "id": 28,
    "bes_tracking_id": 28,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-uncased",
    "organization": "google-bert",
    "description": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/google-bert/bert-base-uncased",
    "model_card": "https://huggingface.co/google-bert/bert-base-uncased",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions of a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google-bert/bert-base-uncased/discussions"
  },
  {
    "id": 29,
    "bes_tracking_id": 29,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-base-openai-detector",
    "organization": "openai-community",
    "description": "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the largest GPT-2 model, the 1.5B parameter version.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "model_card": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "The model is a classifier that can be used to detect text generated by GPT-2 models. However, it is strongly suggested not to use it as a ChatGPT detector for the purposes of making grave allegations of academic misconduct against undergraduates and others, as this model might give inaccurate results in the case of ChatGPT-generated input.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/openai-community/roberta-base-openai-detector/discussions"
  },
  {
    "id": 30,
    "bes_tracking_id": 30,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-cwe-classifier-kelemia",
    "organization": "Dunateo",
    "description": "Kelemia for CWE Classification is a specialized model based on RoBERTa, fine-tuned for classifying software vulnerabilities according to the Common Weakness Enumeration (CWE) standard. This model serves as a critical tool in the security analysis process, achieving robust performance through careful training on relevant datasets. While it offers valuable insights into potential vulnerabilities, it should be utilized as part of a comprehensive security framework rather than a standalone solution, as it may produce false positives or negatives that require validation by security professionals.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "model_card": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "modality": "text; text",
    "analysis": "",
    "size": "125M",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is intended for classifying software vulnerabilities according to the CWE standard. It should be used as part of a broader security analysis process and not as a standalone solution for identifying vulnerabilities.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 31,
    "bes_tracking_id": 31,
    "issue_url": "",
    "type": "LLM",
    "name": "codet5-base",
    "organization": "Salesforce",
    "description": "The model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Salesforce/codet5-base",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/Salesforce/codet5-base",
    "model_card": "https://huggingface.co/Salesforce/codet5-base",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 32,
    "bes_tracking_id": 32,
    "issue_url": "",
    "type": "LLM",
    "name": "gpt-neo-1.3B ",
    "organization": "EleutherAI",
    "description": "GPT-Neo 1.3B is a transformer model developed by EleutherAI, featuring 1.3 billion parameters and based on the GPT-3 architecture. Trained on the diverse Pile dataset, it excels at text generation and predictive tasks. While capable of generating coherent and contextually relevant text, users should be mindful of potential biases and the risk of producing socially unacceptable content.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "model_card": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "modality": "text; text",
    "analysis": "",
    "size": "1.37B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 33,
    "bes_tracking_id": 33,
    "issue_url": "",
    "type": "LLM",
    "name": "detoxify",
    "organization": "unitaryai",
    "description": "Detoxify is a library designed to predict toxic comments using state-of-the-art models trained on multiple Jigsaw challenges. It provides three models: an original model for general toxicity detection, an unbiased model aimed at minimizing identity bias, and a multilingual model capable of classifying comments in seven languages. With recent updates, Detoxify ensures consistent class naming and improved performance metrics, making it a valuable tool for content moderation and research in identifying harmful online content.",
    "created_date": "2024-10-01",
    "model_url": "https://github.com/unitaryai/detoxify",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/unitaryai/detoxify",
    "model_card": "https://github.com/unitaryai/detoxify",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [
      "Transformers",
      "Pytorch Lightning"
    ],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics and/or to aid content moderators in flagging out harmful content quicker.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 34,
    "bes_tracking_id": 34,
    "issue_url": "",
    "type": "LLM",
    "name": "granite-3b-code-base-2k",
    "organization": "ibm-granite",
    "description": "Granite-3B-Code-Base-2K is a decoder-only code model designed for code generative tasks (e.g., code generation, code explanation, code fixing, etc.). It is trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 4 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the modelsâ€™ ability to reason and follow instructions.",
    "created_date": "2024-10-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/ibm-granite/granite-3b-code-base-2k",
    "model_card": "https://huggingface.co/ibm-granite/granite-3b-code-base-2k",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "Prominent enterprise use cases of LLMs in software engineering productivity include code generation, code explanation, code fixing, generating unit tests, generating documentation, addressing technical debt issues, vulnerability detection, code translation, and more. All Granite Code Base models, including the 3B parameter model, are able to handle these tasks as they were trained on a large amount of code data from 116 programming languages.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 35,
    "bes_tracking_id": 35,
    "issue_url": "",
    "type": "LLM",
    "name": "CodeLlama-7b-hf",
    "organization": "codellama",
    "description": "Code Llama is a series of pretrained and fine-tuned generative text models, with sizes ranging from 7 billion to 34 billion parameters. This repository specifically features the 7B instruct-tuned version, optimized for general code synthesis and understanding. The model supports capabilities like code completion, infilling, and instruction following, making it suitable for various programming tasks, particularly in Python.",
    "created_date": "2024-10-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "model_card": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "Fuzz Test"
    ],
    "access": "open",
    "license": "llama2",
    "intended_uses": "Intended Use Cases Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  }
]