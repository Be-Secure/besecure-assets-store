[
  {
    "id": 1,
    "bes_tracking_id": 1,
    "issue_url": "",
    "type": "Classic",
    "name": "bes_image_classification",
    "organization": "Be-Secure",
    "description": "Reference implementations of an MNIST model for image classification, specifically focusing on handwritten digit data. The implementation utilizes the MNIST dataset, which is part of TensorFlow Datasets",
    "created_date": "2023-11-30",
    "model_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/model.zip",
    "data_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/data.zip",
    "label_url": "https://github.com/Be-Secure/bes_image_classification/blob/main/aishield/label.zip",
    "url": "https://github.com/Be-Secure/bes_image_classification",
    "model_card": "https://github.com/Be-Secure/bes_image_classification",
    "modality": "image; image",
    "analysis": "",
    "size": "5.6 MB",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 2,
    "bes_tracking_id": 2,
    "issue_url": "",
    "type": "Classic",
    "name": "structured-data-classification-grn-vsn",
    "organization": "keras-io",
    "description": "This model is built using two important architectural components proposed by Bryan Lim et al. in Temporal Fusion Transformers (TFT) for Interpretable Multi-horizon Time Series Forecasting called GRN and VSN which are very useful for structured data learning tasks.",
    "created_date": "2022-07-04",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for binary classification task to determine whether a person makes over $500K a year or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/structured-data-classification-grn-vsn/discussions"
  },
  {
    "id": 3,
    "bes_tracking_id": 3,
    "issue_url": "",
    "type": "Classic",
    "name": "xgboost-example",
    "organization": "scikit-learn",
    "description": "This is an XGBoost model trained to predict daily alcohol consumption of students.",
    "created_date": "2023-01-20",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/scikit-learn/xgboost-example",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 4,
    "bes_tracking_id": 4,
    "issue_url": "",
    "type": "Classic",
    "name": "TF_Decision_Trees",
    "organization": "keras-io",
    "description": "Build a decision forests model by specifying the input feature usage. Implement a custom Binary Target encoder as a Keras Preprocessing layer to encode the categorical features with respect to their target value co-occurrences, and then use the encoded features to build a decision forests model.",
    "created_date": "2022-02-15",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/TF_Decision_Trees",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/TF_Decision_Trees/discussions"
  },
  {
    "id": 5,
    "bes_tracking_id": 5,
    "issue_url": "",
    "type": "Classic",
    "name": "tab_transformer",
    "organization": "keras-io",
    "description": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "created_date": "2022-06-11",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/tab_transformer",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model can be used for both supervised and semi-supervised tasks on tabular data.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/tab_transformer/discussions"
  },
  {
    "id": 6,
    "bes_tracking_id": 6,
    "issue_url": "",
    "type": "Classic",
    "name": "imbalanced_classification",
    "organization": "keras-io",
    "description": "credit card fraud detection",
    "created_date": "2022-06-13",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/imbalanced_classification",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "The trained model is used to detect of a specific transaction is fraudulent or not.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/imbalanced_classification/discussions"
  },
  {
    "id": 7,
    "bes_tracking_id": 7,
    "issue_url": "",
    "type": "Classic",
    "name": "breast_cancernb8gjv4n-diagnosis-classification",
    "organization": "merve",
    "description": "Baseline Model trained on breast_cancernb8gjv4n to apply classification on diagnosis",
    "created_date": "2022-07-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/merve/breast_cancernb8gjv4n-diagnosis-classification/discussions"
  },
  {
    "id": 8,
    "bes_tracking_id": 8,
    "issue_url": "",
    "type": "Classic",
    "name": "tabular-playground",
    "organization": "demo-org",
    "description": "This is a DecisionTreeClassifier model built for Kaggle Tabular Playground Series August 2022, trained on supersoaker production failures dataset.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/demo-org/tabular-playground",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/demo-org/tabular-playground/discussions"
  },
  {
    "id": 9,
    "bes_tracking_id": 9,
    "issue_url": "",
    "type": "Classic",
    "name": "emb-gam-dino",
    "organization": "Ramos-Ramos",
    "description": "This is a LogisticRegressionCV model trained on averages of patch embeddings from the Imagenette dataset. This forms the GAM of an Emb-GAM extended to images.",
    "created_date": "2022-10-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/Ramos-Ramos/emb-gam-dino",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/Ramos-Ramos/emb-gam-dino/discussions"
  },
  {
    "id": 10,
    "bes_tracking_id": 10,
    "issue_url": "",
    "type": "Classic",
    "name": "ml-generation-failure-prediction",
    "organization": "moro23",
    "description": "ML classification model to predict or identify failures in a generato",
    "created_date": "2022-12-17",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/moro23/ml-generation-failure-prediction",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/moro23/ml-generation-failure-prediction/discussions"
  },
  {
    "id": 11,
    "bes_tracking_id": 11,
    "issue_url": "",
    "type": "Classic",
    "name": "Image_Segmentation",
    "organization": "Be-Secure",
    "description": "Image classification model developed by Be-Secure community. This model trained using the MNIST dataset",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 12,
    "bes_tracking_id": 12,
    "issue_url": "",
    "type": "Classic",
    "name": "Timeseries_Forecasting",
    "organization": "Be-Secure",
    "description": "Reference implementation of an XGBoost Regressor / LSTM model for time series forecasting. The implementation focuses on the House Energy Consumption dataset (Sci-kit learn framework) and the Pump Sensor dataset (TensorFlow model).",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 13,
    "bes_tracking_id": 13,
    "issue_url": "",
    "type": "Classic",
    "name": "Tabular_Classification",
    "organization": "Be-Secure",
    "description": "Reference implementation of an XGBoost model for tabular classification, focused on banking marketing campaign data.The implementation utilizes the banking marketing campaign dataset, which can be downloaded from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/bank+marketing",
    "created_date": "2023-11-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "",
    "modality": "",
    "analysis": "",
    "size": "Not Applicable",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST",
      "Fuzz Test"
    ],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 14,
    "bes_tracking_id": 14,
    "issue_url": "",
    "type": "LLM",
    "name": "MathCoder",
    "organization": "Shanghai AI Laboratory",
    "description": "MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.",
    "created_date": "2023-10-05",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": [
      "GPT-4",
      "LLaMA 2"
    ],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 15,
    "bes_tracking_id": 15,
    "issue_url": "",
    "type": "LLM",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.",
    "created_date": "2023-09-06",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://falconllm.tii.ae/falcon-models.html",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": [
      "RefinedWeb"
    ],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": [],
    "access": "open",
    "license": "",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "id": 16,
    "bes_tracking_id": 16,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-18",
    "organization": "microsoft",
    "description": "ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-18",
    "modality": "",
    "analysis": "",
    "size": "11.7M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 17,
    "bes_tracking_id": 17,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-arabic-camelbert-ca-pos-glf",
    "organization": "CAMeL-Lab",
    "description": "CAMeLBERT-CA POS-GLF Model is a Gulf Arabic POS tagging model that was built by fine-tuning the CAMeLBERT-CA model. For the fine-tuning, we used the Gumar dataset.",
    "created_date": "2021-10-18",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-ca-pos-glf",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 18,
    "bes_tracking_id": 18,
    "issue_url": "",
    "type": "LLM",
    "name": "all-MiniLM-L6-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-08-31",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "1B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "as a sentence and short paragraph encoder",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 19,
    "bes_tracking_id": 19,
    "issue_url": "",
    "type": "LLM",
    "name": "paraphrase-multilingual-MiniLM-L12-v2",
    "organization": "sentence-transformers",
    "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
    "created_date": "2021-06-09",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 20,
    "bes_tracking_id": 20,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-NER",
    "organization": "dslim",
    "description": "bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC).",
    "created_date": "2020-12-12",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/dslim/bert-base-NER",
    "modality": "",
    "analysis": "",
    "size": "108M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "mit",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 21,
    "bes_tracking_id": 21,
    "issue_url": "",
    "type": "LLM",
    "name": "vit-base-patch16-224",
    "organization": "google",
    "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch.",
    "created_date": "2021-03-24",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/google/vit-base-patch16-224",
    "modality": "image; image",
    "analysis": "",
    "size": "86.6M parameters",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 22,
    "bes_tracking_id": 22,
    "issue_url": "",
    "type": "LLM",
    "name": "resnet-50",
    "organization": "microsoft",
    "description": "VResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.",
    "created_date": "2022-06-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/microsoft/resnet-50",
    "modality": "image; image",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "You can use the raw model for image classification",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 23,
    "bes_tracking_id": 23,
    "issue_url": "",
    "type": "LLM",
    "name": "collaborative-filtering-movielens",
    "organization": "keras-io",
    "description": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "created_date": "2022-06-02",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/keras-io/collaborative-filtering-movielens",
    "modality": "",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "CC0 1.0",
    "intended_uses": "Based on a user and movies they have rated highly in the past, this model outputs the predicted rating a user would give to a movie they haven't seen yet (between 0-1). This information can be used to find out the top recommended movies for this user.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/keras-io/collaborative-filtering-movielens/discussions"
  },
  {
    "id": 24,
    "bes_tracking_id": 24,
    "issue_url": "",
    "type": "Classic",
    "name": "modeltest",
    "organization": "vendorabc",
    "description": "This is a HistGradientBoostingClassifier model trained on breast cancer dataset. It's trained with Halving Grid Search Cross Validation, with parameter grids on max_leaf_nodes and max_depth.",
    "created_date": "2022-08-30",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "",
    "model_card": "https://huggingface.co/vendorabc/modeltest",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [
      "SAST"
    ],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/vendorabc/modeltest/discussions"
  },
  {
    "id": 25,
    "bes_tracking_id": 25,
    "issue_url": "",
    "type": "LLM",
    "name": "deberta-v3-base-prompt-injection-v2",
    "organization": "protectai",
    "description": "This model is a fine-tuned version of microsoft/deberta-v3-base specifically developed to detect and classify prompt injection attacks which can manipulate language models into producing unintended outputs.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "model_card": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "This model is not ready to be used in production.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2/discussions"
  },
  {
    "id": 26,
    "bes_tracking_id": 26,
    "issue_url": "",
    "type": "LLM",
    "name": "flan-t5-xxl",
    "organization": "google",
    "description": "These models have been fine-tuned on more than 1000 additional tasks covering also more languages. As mentioned in the first few lines of the abstrac",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/google/flan-t5-xxl",
    "model_card": "https://huggingface.co/google/flan-t5-xxl",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The model is designed for tasks where only a few or no examples are provided during fine-tuning, such as reasoning and question-answering tasks.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google/flan-t5-xxl/discussions"
  },
  {
    "id": 27,
    "bes_tracking_id": 27,
    "issue_url": "",
    "type": "LLM",
    "name": "distilbert-base-uncased-finetuned-sst-2-english",
    "organization": "distilbert",
    "description": "This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2. This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "model_card": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "This model can be used for topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/discussions"
  },
  {
    "id": 28,
    "bes_tracking_id": 28,
    "issue_url": "",
    "type": "LLM",
    "name": "bert-base-uncased",
    "organization": "google-bert",
    "description": "BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/google-bert/bert-base-uncased",
    "model_card": "https://huggingface.co/google-bert/bert-base-uncased",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache-2.0",
    "intended_uses": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions of a task that interests you.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/google-bert/bert-base-uncased/discussions"
  },
  {
    "id": 29,
    "bes_tracking_id": 29,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-base-openai-detector",
    "organization": "openai-community",
    "description": "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the largest GPT-2 model, the 1.5B parameter version.",
    "created_date": "2024-09-1",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "model_card": "https://huggingface.co/openai-community/roberta-base-openai-detector",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "The model is a classifier that can be used to detect text generated by GPT-2 models. However, it is strongly suggested not to use it as a ChatGPT detector for the purposes of making grave allegations of academic misconduct against undergraduates and others, as this model might give inaccurate results in the case of ChatGPT-generated input.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/openai-community/roberta-base-openai-detector/discussions"
  },
  {
    "id": 30,
    "bes_tracking_id": 30,
    "issue_url": "",
    "type": "LLM",
    "name": "roberta-cwe-classifier-kelemia",
    "organization": "Dunateo",
    "description": "This model is a fine-tuned version of RoBERTa for classifying Common Weakness Enumeration (CWE) vulnerabilities.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "model_card": "https://huggingface.co/Dunateo/roberta-cwe-classifier-kelemia",
    "modality": "text; text",
    "analysis": "",
    "size": "125M",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This model is intended for classifying software vulnerabilities according to the CWE standard. It should be used as part of a broader security analysis process and not as a standalone solution for identifying vulnerabilities.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 31,
    "bes_tracking_id": 31,
    "issue_url": "",
    "type": "LLM",
    "name": "codet5-base",
    "organization": "Salesforce",
    "description": "The model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/Salesforce/codet5-base",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/Salesforce/codet5-base",
    "model_card": "https://huggingface.co/Salesforce/codet5-base",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 32,
    "bes_tracking_id": 32,
    "issue_url": "",
    "type": "LLM",
    "name": "gpt-neo-1.3B ",
    "organization": "EleutherAI",
    "description": "GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.",
    "created_date": "2024-10-01",
    "model_url": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "model_card": "https://huggingface.co/EleutherAI/gpt-neo-1.3B",
    "modality": "text; text",
    "analysis": "",
    "size": "1.37B",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "MIT",
    "intended_uses": "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 33,
    "bes_tracking_id": 33,
    "issue_url": "",
    "type": "LLM",
    "name": "detoxify",
    "organization": "unitaryai",
    "description": "Toxic Comment Classification with Pytorch Lightning and Transformers",
    "created_date": "2024-10-01",
    "model_url": "https://github.com/unitaryai/detoxify",
    "data_url": "",
    "label_url": "",
    "url": "https://github.com/unitaryai/detoxify",
    "model_card": "https://github.com/unitaryai/detoxify",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [
      "Transformers",
      "Pytorch Lightning"
    ],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": [],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "The intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics and/or to aid content moderators in flagging out harmful content quicker.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 34,
    "bes_tracking_id": 34,
    "issue_url": "",
    "type": "LLM",
    "name": "granite-3b-code-base-2k",
    "organization": "ibm-granite",
    "description": "Granite-3B-Code-Base-2K is a decoder-only code model designed for code generative tasks (e.g., code generation, code explanation, code fixing, etc.). It is trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 4 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the modelsâ€™ ability to reason and follow instructions.",
    "created_date": "2024-10-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/ibm-granite/granite-3b-code-base-2k",
    "model_card": "https://huggingface.co/ibm-granite/granite-3b-code-base-2k",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Insecure Code Detection", "LLM Benchmark"],
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "Prominent enterprise use cases of LLMs in software engineering productivity include code generation, code explanation, code fixing, generating unit tests, generating documentation, addressing technical debt issues, vulnerability detection, code translation, and more. All Granite Code Base models, including the 3B parameter model, are able to handle these tasks as they were trained on a large amount of code data from 116 programming languages.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  },
  {
    "id": 35,
    "bes_tracking_id": 35,
    "issue_url": "",
    "type": "LLM",
    "name": "CodeLlama-7b-hf",
    "organization": "codellama",
    "description": "Code Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 7B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding.",
    "created_date": "2024-10-07",
    "model_url": "",
    "data_url": "",
    "label_url": "",
    "url": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "model_card": "https://huggingface.co/codellama/CodeLlama-7b-hf",
    "modality": "text; text",
    "analysis": "",
    "size": "",
    "dependencies": [],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": ["Insecure Code Detection", "LLM Benchmark"],
    "access": "open",
    "license": "llama2",
    "intended_uses": "Intended Use Cases Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  }
]
